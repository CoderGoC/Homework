{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"california.csv\")\n",
    "data.head()\n",
    "X = data.drop(columns=[\"Unnamed: 0\",'PRICE'])\n",
    "y = data['PRICE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.7273129773603115\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "RMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"RMSE: \", RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial CV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE scores for each fold:  [1.75756219 0.66482411 0.67856784 0.65015863 1.69901943 0.71105288\n",
      " 2.14170892 0.68631593 0.65826183 0.61060377]\n",
      "Mean RMSE:  1.0258075536400146\n"
     ]
    }
   ],
   "source": [
    "degree = 2   \n",
    "poly = PolynomialFeatures(degree)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "RMSEs = np.sqrt(-cross_val_score(model, X_poly, y, cv=kf, scoring='neg_mean_squared_error'))\n",
    "\n",
    "print(\"RMSE scores for each fold: \", RMSEs)\n",
    "print(\"Mean RMSE: \", np.mean(RMSEs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression and Poly Ridge Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.7581994998956556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.00047e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n"
     ]
    }
   ],
   "source": [
    "degree = 2   \n",
    "poly = PolynomialFeatures(degree)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.3, random_state=0)\n",
    "\n",
    "ridge_model = Ridge(alpha=100)\n",
    "\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = ridge_model.predict(X_test)\n",
    "\n",
    "RMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"RMSE: \", RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 100\n",
      "\n",
      "RMSE scores for each fold:  [0.74752735 0.73668678 0.69452319 0.75770788 0.70067292 0.72805361\n",
      " 0.74577786 0.68950359 0.74958641 0.7426455 ]\n",
      "\n",
      "Mean RMSE:  0.7292685096844017\n"
     ]
    }
   ],
   "source": [
    "degree = 2   \n",
    "poly = PolynomialFeatures(degree)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.3, random_state=0)\n",
    "\n",
    "ridge_model = Ridge(alpha=100)\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "RMSEs = np.sqrt(-cross_val_score(ridge_model, X, y, cv=kf, scoring='neg_mean_squared_error'))\n",
    "\n",
    "print(\"Alpha: 100\")\n",
    "print()\n",
    "print(\"RMSE scores for each fold: \", RMSEs)\n",
    "print()\n",
    "print(\"Mean RMSE: \", np.mean(RMSEs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.28210271e-01,  9.59212473e-03, -9.10124804e-02,  5.58249404e-01,\n",
       "       -3.39287716e-06, -3.77260928e-03, -4.19061126e-01, -4.30992513e-01])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_model = Ridge(alpha=100)\n",
    "ridge_model.fit(X, y)\n",
    "ridge_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Regression and Poly Lasso Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression with Train-Test Split\n",
      "\n",
      "Alpha: 1.0\n",
      "\n",
      "RMSE:  2.2831710430287666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.583e+03, tolerance: 1.923e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "degree = 2   \n",
    "poly = PolynomialFeatures(degree)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.3, random_state=0)\n",
    "\n",
    "lasso_model = Lasso(alpha=1.0)\n",
    "\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lasso_model.predict(X_test)\n",
    "\n",
    "RMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"Lasso Regression with Train-Test Split\")\n",
    "print()\n",
    "print(\"Alpha: 1.0\")\n",
    "print()\n",
    "print(\"RMSE: \", RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression with Cross-Validation\n",
      "Alpha: 1.0\n",
      "RMSE scores for each fold:  [0.74556736 0.72640039 0.71364566 0.7105335  0.7451019 ]\n",
      "Mean RMSE:  0.7282497615433826\n"
     ]
    }
   ],
   "source": [
    "degree = 2   \n",
    "poly = PolynomialFeatures(degree)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "lasso_model = Lasso(alpha=0.00001)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "RMSEs = np.sqrt(-cross_val_score(lasso_model, X, y, cv=kf, scoring='neg_mean_squared_error'))\n",
    "\n",
    "print(\"Lasso Regression with Cross-Validation\")\n",
    "print(\"Alpha: 1.0\")\n",
    "print(\"RMSE scores for each fold: \", RMSEs)\n",
    "print(\"Mean RMSE: \", np.mean(RMSEs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.36324937e-01,  9.44260979e-03, -1.06612259e-01,  6.41358357e-01,\n",
       "       -3.94906978e-06, -3.78503762e-03, -4.21193206e-01, -4.34335769e-01])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_model = Lasso(alpha=0.0001)\n",
    "lasso_model.fit(X, y)\n",
    "lasso_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.1\n",
      "RMSE scores for each fold:  [0.72731439 0.73557105 0.7346968  0.73002939 0.70405246]\n",
      "Mean RMSE:  0.7263328163214797\n",
      "--------------------------------------------\n",
      "Alpha: 1\n",
      "RMSE scores for each fold:  [0.72732711 0.735563   0.73469594 0.7299813  0.7040628 ]\n",
      "Mean RMSE:  0.7263260307207104\n",
      "--------------------------------------------\n",
      "Alpha: 10\n",
      "RMSE scores for each fold:  [0.72745456 0.73548724 0.73469115 0.72951999 0.70416632]\n",
      "Mean RMSE:  0.7262638521162934\n",
      "--------------------------------------------\n",
      "Alpha: 100\n",
      "RMSE scores for each fold:  [0.72872419 0.73509051 0.73491706 0.7263745  0.7051864 ]\n",
      "Mean RMSE:  0.726058531681262\n",
      "--------------------------------------------\n",
      "Alpha: 1000\n",
      "RMSE scores for each fold:  [0.73798719 0.73889973 0.74135124 0.72569698 0.71277999]\n",
      "Mean RMSE:  0.7313430274291182\n",
      "--------------------------------------------\n",
      "Alpha: 10000\n",
      "RMSE scores for each fold:  [0.78259857 0.77863332 0.78431429 0.76886833 0.75756646]\n",
      "Mean RMSE:  0.7743961919785891\n",
      "--------------------------------------------\n",
      "Alpha: 100000\n",
      "RMSE scores for each fold:  [0.94325851 0.95212218 0.95289615 0.94482908 0.93014516]\n",
      "Mean RMSE:  0.9446502146413666\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "degree = 2   \n",
    "poly = PolynomialFeatures(degree)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "alphas = [0.1, 1, 10, 100, 1_000, 10_000, 100_000]\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge_model = Ridge(alpha=alpha)\n",
    "    \n",
    "    RMSEs = np.sqrt(-cross_val_score(ridge_model, X, y, cv=kf, scoring='neg_mean_squared_error'))\n",
    "    \n",
    "    print(f\"Alpha: {alpha}\")\n",
    "    print(\"RMSE scores for each fold: \", RMSEs)\n",
    "    print(\"Mean RMSE: \", np.mean(RMSEs))\n",
    "    print(\"-\" * 44)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ElasticNet HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.767e+03, tolerance: 2.210e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.765e+03, tolerance: 2.186e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.755e+03, tolerance: 2.188e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.805e+03, tolerance: 2.197e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.841e+03, tolerance: 2.212e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.271e+03, tolerance: 2.210e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.279e+03, tolerance: 2.186e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.260e+03, tolerance: 2.188e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.307e+03, tolerance: 2.197e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.343e+03, tolerance: 2.212e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.319e+03, tolerance: 2.210e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.267e+03, tolerance: 2.186e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.1, 1.0, 10.0, 100, 1000, 10000]\n",
    "l1_ratios = [0, 0.1, 0.5, 0.9]\n",
    "\n",
    "degree = 2   \n",
    "poly = PolynomialFeatures(degree)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "natija = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    for l1_ratio in l1_ratios:\n",
    "\n",
    "        elasticnet_model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "        \n",
    "\n",
    "        RMSEs = np.sqrt(-cross_val_score(elasticnet_model, X, y, cv=kf, scoring='neg_mean_squared_error'))\n",
    "        \n",
    "\n",
    "        natija.append({\n",
    "            'alpha': alpha,\n",
    "            'l1_ratio': l1_ratio,\n",
    "            'mean_rmse': np.mean(RMSEs),\n",
    "            'std_rmse': np.std(RMSEs),\n",
    "            'rmse_scores': RMSEs\n",
    "        })\n",
    "\n",
    "\n",
    "natijalar = pd.DataFrame(natija)\n",
    "\n",
    "natijalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Umumiy xulosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Umumiy xulosa qiladigan bolsak biz uchun eng yaxshi model chiqizlik model bolyapti chunki undagi RMSE eng kam bolib qolmoqda. Lekin outfitting hisobiga undagi barqarorlik kam bolishi mukin. Shuning uchun bashorat model sifatida biz Hyperparameter choosing orqali korgan alpha 100 ga teng bolgandagi Ridge model eng yaxshisi bolishi mumkin, bunda biz CV hamda PolyFuatures dan foydalandik"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
